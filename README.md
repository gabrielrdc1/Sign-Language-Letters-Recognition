Sign Language Letters Recognition Using Image Processing and Deep Learning

This project focuses on recognizing hand signs representing letters of the Brazilian Sign Language (Libras) alphabet using Python, image processing, and deep learning techniques. The system utilizes TensorFlow, OpenCV, and Google’s Teachable Machine for training and implementation.

Features
Programming Language: Developed in Python, leveraging its versatility and robust libraries for machine learning and image processing.

Dataset: Approximately 250 images per class, covering all 26 letters of the alphabet, resulting in a well-balanced dataset for training.

Training: The model was trained using Google’s Teachable Machine, streamlining the training process while ensuring high performance.

Image Processing: OpenCV was used for preprocessing, including resizing, color adjustments, and data normalization, enhancing the model’s accuracy.

Prediction: A TensorFlow-based classifier predicts the corresponding letter from hand gesture inputs with reliable precision.

Goals
This project aims to:

Enhance accessibility for individuals with hearing impairments by facilitating seamless communication through sign language recognition.

Provide a solid foundation for future research and advancements in sign language and gesture recognition technologies.
